{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06808735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec\n",
      "loading complete\n",
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from num2words import num2words\n",
    "\n",
    "POS_LIST = ['n','v','a','s','r']\n",
    "\n",
    "print('loading word2vec')\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "print('loading complete')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using device',device)\n",
    "\n",
    "def get_synonyms_antonyms(word):\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None,None\n",
    "    \n",
    "    synonyms = set()\n",
    "    antonyms = set()\n",
    "\n",
    "    for syn in synsets:\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name()\n",
    "            if synonym.isalnum():\n",
    "                synonyms.add(l.name().lower())\n",
    "            if l.antonyms():\n",
    "                antonym = l.antonyms()[0].name()\n",
    "                if antonym.isalnum():\n",
    "                    antonyms.add(antonym.lower())\n",
    "    synonyms.discard(word)\n",
    "    return list(synonyms), list(antonyms)\n",
    "    \n",
    "def get_pos(word):\n",
    "    total = 0\n",
    "    \n",
    "    out = torch.tensor(np.zeros(len(POS_LIST)),dtype=torch.float)\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    pos = Counter()\n",
    "    for syn in synsets:\n",
    "        pos[syn.pos()]+=1\n",
    "        total += 1\n",
    "    for i,a_pos in enumerate(POS_LIST):\n",
    "        out[i] = pos[a_pos]/total\n",
    "    return out\n",
    "\n",
    "def get_hypernyms_hyponyms(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None,None\n",
    "    \n",
    "    hypernyms,hyponyms = set(),set()\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            candidate = hyper.name().split('.')[0]\n",
    "            if candidate.isalnum():\n",
    "                hypernyms.add(candidate.lower())\n",
    "        \n",
    "        for hypo in syn.hyponyms():\n",
    "            candidate = hypo.name().split('.')[0]\n",
    "            if candidate.isalnum():\n",
    "                hyponyms.add(candidate.lower())\n",
    "                \n",
    "    return list(hypernyms),list(hyponyms)\n",
    "\n",
    "def get_word_data(word):\n",
    "    # return synonyms, antonyms, hypernyms, hyponyms, pos vector\n",
    "    aux_vocab = set()\n",
    "    synonyms, antonyms = get_synonyms_antonyms(word)\n",
    "    hypernyms,hyponyms = get_hypernyms_hyponyms(word)\n",
    "    pos_vector = get_pos(word)\n",
    "    \n",
    "    for word in chain(synonyms or [],antonyms or [],hypernyms or [],hyponyms or []):\n",
    "        aux_vocab.add(word)\n",
    "    return synonyms, antonyms, hypernyms,hyponyms,pos_vector, aux_vocab\n",
    "\n",
    "def enrich_with_aux_vocab(w2i,i2w,i2v,i2data, aux_vocab_total,vocab):\n",
    "    i = len(i2v)\n",
    "    for w in aux_vocab_total:\n",
    "        if not w in vocab:\n",
    "            w2i[w] = i\n",
    "            i += 1\n",
    "            if w in wv:\n",
    "                i2v.append(wv[w])\n",
    "            else:\n",
    "                i2v.append(torch.tensor(np.zeros(NUM_INPUT),dtype=torch.float))\n",
    "            i2data.append(None)\n",
    "    for word in aux_vocab_total:\n",
    "        vocab.add(word)\n",
    "\n",
    "\n",
    "def get_vector_vocab(vocab):\n",
    "    w2i = dict()\n",
    "    i2w = list(vocab)\n",
    "    i2v = list()\n",
    "    i2data = list()\n",
    "    aux_vocab_total = set()\n",
    "    for i,w in enumerate(i2w):\n",
    "        w2i[w] = i\n",
    "        if w in wv:\n",
    "            i2v.append(wv[w])\n",
    "            synonyms, antonyms, hypernyms,hyponyms,pos_vector, aux_vocab = get_word_data(w)\n",
    "            for aux_w in aux_vocab:\n",
    "                aux_vocab_total.add(aux_w)\n",
    "            i2data.append((synonyms, antonyms, hypernyms,hyponyms,pos_vector))\n",
    "        else:\n",
    "            i2v.append([0 for _ in range(NUM_INPUT)])\n",
    "            i2data.append((None,None,None,None,None))\n",
    "    input_vocab = torch.tensor(i2v,dtype=torch.float)\n",
    "    enrich_with_aux_vocab(w2i,i2w,i2v,i2data, aux_vocab_total,vocab)\n",
    "    return w2i,i2w,i2v,i2data,input_vocab\n",
    "\n",
    "def w2t(word,w2i,i2t):\n",
    "    if not word or not word in w2i:\n",
    "        return None\n",
    "    return i2t[w2i[word]]\n",
    "\n",
    "def sample_choice(group_a,group_b,w2i,i2t):\n",
    "    if np.random.random()<0.5 and group_a:\n",
    "        return w2t(random.choice(group_a),w2i,i2t),torch.tensor([1],dtype=torch.float)\n",
    "    elif group_b:\n",
    "        return w2t(random.choice(group_b),w2i,i2t),torch.tensor([0],dtype=torch.float)\n",
    "    return None,None\n",
    "\n",
    "class EmbeddingModifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingModifierNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(NUM_INPUT,NUM_HIDDEN)\n",
    "        \n",
    "        self.equality_layer = nn.Linear(NUM_HIDDEN,NUM_INPUT)\n",
    "        self.synonym_antonym_layer = nn.Linear(NUM_HIDDEN+NUM_INPUT,1)\n",
    "        self.hypernym_hyponym_layer = nn.Linear(NUM_HIDDEN+NUM_INPUT,1)\n",
    "        self.pos_layer = nn.Linear(NUM_HIDDEN,len(POS_LIST))\n",
    "\n",
    "    def forward(self,x,syn_or_ant,hyper_or_hypo):\n",
    "        x = self.hidden(x)\n",
    "        \n",
    "        # main output layer\n",
    "        main_out = self.equality_layer(x)\n",
    "        main_out = torch.sigmoid(main_out)\n",
    "        \n",
    "        syn_ant_out = None\n",
    "        if syn_or_ant is not None:\n",
    "            syn_ant_out = self.synonym_antonym_layer(torch.hstack([x,syn_or_ant]))\n",
    "            syn_ant_out = torch.sigmoid(syn_ant_out)\n",
    "        \n",
    "        hyper_hypo_out = None\n",
    "        if hyper_or_hypo is not None:\n",
    "            hyper_hypo_out = self.synonym_antonym_layer(torch.hstack([x,hyper_or_hypo]))\n",
    "            hyper_hypo_out = torch.sigmoid(hyper_hypo_out)\n",
    "            \n",
    "        pos_out = self.pos_layer(x)\n",
    "        pos_out = torch.softmax(pos_out,dim=0)\n",
    "        \n",
    "        return main_out,syn_ant_out,hyper_hypo_out,pos_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f114257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('vocab.dill', 'rb') as file:\n",
    "    vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856b9710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1e1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-fc6ad6a643c4>:119: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  input_vocab = torch.tensor(i2v,dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT = 300\n",
    "\n",
    "w2i,i2w,i2v,i2data,input_vocab = get_vector_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6a7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2t = torch.tensor(i2v,dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6cd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize for num input = 300\n",
    "\n",
    "with open('w2i.dill', 'wb') as file:\n",
    "    pickle.dump(w2i, file)\n",
    "    \n",
    "with open('i2w.dill', 'wb') as file:\n",
    "    pickle.dump(i2w, file)\n",
    "    \n",
    "with open('i2data.dill', 'wb') as file:\n",
    "    pickle.dump(i2data, file)\n",
    "    \n",
    "torch.save(i2t,'word2vec_embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2i.dill', 'rb') as file:\n",
    "    w2i = pickle.load(file)\n",
    "    \n",
    "with open('i2w.dill', 'rb') as file:\n",
    "    i2w = pickle.load(file)\n",
    "    \n",
    "with open('i2data.dill', 'rb') as file:\n",
    "    i2data = pickle.load(file)\n",
    "    \n",
    "embeds = torch.load('word2vec_embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa2aea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4eb104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3125, -0.2129,  0.0874,  ..., -0.0300,  0.1523,  0.0659],\n",
       "        [ 0.0483,  0.1279,  0.1064,  ..., -0.1084,  0.1582, -0.0469],\n",
       "        [ 0.1582,  0.2734, -0.1963,  ...,  0.1143,  0.0767, -0.0051],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0400, -0.0038,  0.0034,  ..., -0.2119,  0.2812,  0.3301]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c9032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
